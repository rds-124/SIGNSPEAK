# ğŸ¤Ÿ SIGNSPEAK: Bridging Gestures, Speech, and Translation ğŸŒğŸ—£ï¸

![Python](https://img.shields.io/badge/Python-3.10-blue)
![TensorFlow](https://img.shields.io/badge/TensorFlow-Used-orange)
![MediaPipe](https://img.shields.io/badge/MediaPipe-Used-success)
![SpeechRecognition](https://img.shields.io/badge/SpeechRecognition-Enabled-green)
![Google Translate](https://img.shields.io/badge/Translation-Multilingual-informational)

> Translate hand gestures into spoken words in **any language** using AI, deep learning, and computer vision.

---

## ğŸš€ About the Project

**SIGNSPEAK** is an AI-powered system that:

* ğŸ“· Detects hand gestures in real-time using your webcam
* ğŸ¤– Classifies them using a trained deep learning model
* ğŸŒ Translates the recognized gesture to your chosen language
* ğŸ—£ï¸ Speaks the translated output using text-to-speech

It serves as an assistive tech for **speech-impaired individuals** or a demo combining computer vision, NLP, and TTS.

---

## ğŸ¯ Key Features

* âœ… Real-time hand gesture recognition (MediaPipe + OpenCV)
* ğŸŒ Multilingual translation with `deep_translator`
* ğŸ—£ï¸ Offline text-to-speech (pyttsx3)
* ğŸ§  Custom-trained model for gesture recognition
* ğŸ” CLI-based easy language selection

---

## ğŸ“š Project Structure

```
SIGNSPEAK/
â”œâ”€â”€ hand_gesture_translator.py     # Main script
â”œâ”€â”€ gesture.names                  # Gesture class labels
â”œâ”€â”€ mp_hand_gesture/               # Trained model folder
â”œâ”€â”€ data.pickle                    # Training dataset (landmarks)
â”œâ”€â”€ requirements.txt               # Dependencies
â”œâ”€â”€ .gitignore                     # Ignore unnecessary files
â””â”€â”€ README.md                      # Project documentation
```

---

## ğŸ“¦ Installation

1. **Clone the Repository**

```bash
git clone https://github.com/rds-124/SIGNSPEAK.git
cd SIGNSPEAK
```

2. **Install Dependencies**

```bash
pip install -r requirements.txt
```

3. **Run the Application**

```bash
python hand_gesture_translator.py
```

---

## ğŸŒ Language Codes (for Translation)

| Language | Code |
| -------- | ---- |
| Spanish  | `es` |
| Hindi    | `hi` |
| Tamil    | `ta` |
| Kannada  | `kn` |
| German   | `de` |
| Japanese | `ja` |
| French   | `fr` |
| Telugu   | `te` |
| English  | `en` |

Find more codes [here](https://cloud.google.com/translate/docs/languages).

---

## ğŸ§  Model Details

* Built with MediaPipe for extracting hand landmark data
* Classifies gestures using a simple dense neural network
* Supports training on custom data (`data.pickle` file)

---

## ğŸ“ˆ Future Work

* ğŸ¤ Voice-controlled language change
* ğŸ“² Mobile deployment with TensorFlow Lite
* ğŸ”§ GUI Interface with Tkinter or PyQt
* âš¡ Gesture dataset expansion with augmentation

---

## ğŸ™ Acknowledgements

* [MediaPipe](https://mediapipe.dev/) by Google
* [TensorFlow](https://www.tensorflow.org/)
* [deep\_translator](https://github.com/nidhaloff/deep-translator)
* [pyttsx3](https://pypi.org/project/pyttsx3/)

---

## ğŸ“… License

This project is licensed under the **MIT License**.

---

## ğŸ“§ Contact

**Rohith D**
[LinkedIn](https://www.linkedin.com/in/rohith124) | [GitHub](https://github.com/rds-124)

> â­ If you found this useful, star the repo and share it!
